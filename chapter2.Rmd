---
title: "Chapter 2: Statistical Learning"
output: html_notebook
---

***
***
## Intro to R
```{r}
x <- c(1, 3, 2, 5)
x
```

```{r}
x = c(1, 6, 2)
y = c(1, 4, 3)
length(x)
```
```{r}
ls()
```


```{r}
rm(x, y)
ls()
rm(list=ls()) # remove all
```


```{r}
?matrix
x = matrix(data=c(1,2,3,4), nrow=2, ncol=2)
x = matrix(c(1,2,3,4) ,2,2)
x
```


```{r}
matrix(c(1,2,3,4), 2, 2, byrow=TRUE)
```


```{r}
sqrt(x)
```


```{r}
x^2
```


```{r}
set.seed(1303) # seed randomness
x = rnorm(50)
y = x + rnorm(50, mean=50, sd=0.1)
cor(x, y)
```


```{r}
set.seed(3)
y = rnorm(100)
mean(y)
```


```{r}
print(var(y))
print(sqrt(var(y)) == sd(y))
```


```{r}
x = rnorm(100)
y = rnorm(100)
plot(x,y)
plot(x,y,xlab="this is the x-axis",ylab="this is the y-axis", main="Plot of X vs Y")
```


```{r}
x = seq(1, 10)
print(x)
x = 1:10
print(x)

```


```{r}
y = x
f = outer(x, y, function(x,y) cos(y)/(1+x^2))
contour(x, y, f)
contour(x, y, f, nlevels=45, add=T)
fa = (f-t(f))/2 # t() is transform
contour(x, y, fa, nlevels=15)
```


```{r}
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta=30)
persp(x, y, fa, theta=30, phi=20)
persp(x, y, fa, theta=30, phi=70)
persp(x, y, fa, theta=30, phi=40)
```


```{r}
A = matrix(1:16, 4, 4)
A
```


```{r}
A[2, 3]
```


```{r}
A[c(1,3), c(2,4)]
```


```{r}
print(A[, 1:2])
print(A[1,])
```


```{r}
A[-c(1,3), ] # negatives mean exclude
```


```{r}
dim(A)
```


```{r}
library(MASS)
library(ISLR) # installed manually with Tools -> Install Package
?Auto
Auto
```


```{r}
Auto = na.omit(Auto) # seems to already have happened on load
print(dim(Auto))
names(Auto)
```


```{r}
plot(Auto$cylinders, Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
```


```{r}
cylinders = as.factor(cylinders) # quantitative -> qualitative (classes/categories)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red ")
plot(cylinders, mpg, col="red", varwidth=T)
plot(cylinders, mpg, col="red", varwidth, horizontal=T)
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders ", ylab="MPG")
```


```{r}
hist(mpg)
hist(mpg, col=2)
hist(mpg, col=2, breaks=15)
```


```{r}
?pairs
pairs(Auto)
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
```


```{r}
plot(horsepower, mpg)
identify(horsepower, mpg, name) # makes interactive - doesn't work in R Markdown
```


```{r}
summary(Auto)
```


```{r}
summary(mpg)
```

***
***
## Exercises
1)
  a) Flexible model better as more n wouldn't likely drastically change $\hat{f}$ (low variance) and less flexible models may have too much bias
  b) Less flexible model better as less likely to overfit (less likely to have high variance)
  c) Flexible model better as more likely to have low bias
  d) Less flexible model better as less likely to overfit the high error terms
  
2)
  a) Regression, inference, n=500, p=3
  b) Classification, prediction, n=20, p=13
  c) Regression, prediction, n=52, p=3

```{r}
College
```


```{r}
summary(College)
```


```{r}
pairs(College[, 1:10])
```


```{r}
attach(College)
boxplot(Outstate~Private, xlab="Private", ylab="Outstate")
```


```{r}
Elite = rep("No", nrow(College))
Elite[Top10perc>50] = "Yes"
Elite = as.factor(Elite)
College = data.frame(College, Elite)
summary(Elite)
```


```{r}
boxplot(Outstate~Elite)
```


```{r}
print(names(College))
hist(Top10perc, breaks=5)
```


```{r}
sapply(Auto, class)
```


```{r}
qualAuto = Auto[, -ncol(Auto)]
sapply(qualAuto, range)
```


```{r}
sapply(qualAuto, mean)
sapply(qualAuto, sd)
```


```{r}
sapply(qualAuto[-(10:85),], mean)
sapply(qualAuto[-(10:85),], sd)
```


```{r}
pairs(qualAuto)
```


```{r}
dim(Boston)
```


```{r}
pairs(Boston)
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```

